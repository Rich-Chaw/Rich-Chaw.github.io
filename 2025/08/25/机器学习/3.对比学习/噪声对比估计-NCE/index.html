<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>噪声对比估计-NCE | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="NCE目标函数：$$\begin{aligned}\max_\theta J^c(\theta)&amp;&#x3D;\max_\theta \mathbb{E}X[log P\theta( D | w , θ)]\&amp;&#x3D;\max_\theta \left(\mathbb{E}{P(w|c)}\left[\log\frac{P\theta(w|c)}{P_\theta(w|c)+k">
<meta property="og:type" content="article">
<meta property="og:title" content="噪声对比估计-NCE">
<meta property="og:url" content="http://example.com/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E5%99%AA%E5%A3%B0%E5%AF%B9%E6%AF%94%E4%BC%B0%E8%AE%A1-NCE/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="NCE目标函数：$$\begin{aligned}\max_\theta J^c(\theta)&amp;&#x3D;\max_\theta \mathbb{E}X[log P\theta( D | w , θ)]\&amp;&#x3D;\max_\theta \left(\mathbb{E}{P(w|c)}\left[\log\frac{P\theta(w|c)}{P_\theta(w|c)+k">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-08-25T02:08:52.000Z">
<meta property="article:modified_time" content="2025-08-25T02:50:31.389Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-机器学习/3.对比学习/噪声对比估计-NCE" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E5%99%AA%E5%A3%B0%E5%AF%B9%E6%AF%94%E4%BC%B0%E8%AE%A1-NCE/" class="article-date">
  <time class="dt-published" datetime="2025-08-25T02:08:52.000Z" itemprop="datePublished">2025-08-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>►<a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">3.对比学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      噪声对比估计-NCE
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>NCE目标函数：<br>$$\begin{aligned}\max_\theta J^c(\theta)&amp;&#x3D;\max_\theta \mathbb{E}<em>X[log P</em>\theta( D | w , θ)]<br>\&amp;&#x3D;\max_\theta \left(\mathbb{E}<em>{P(w|c)}\left[\log\frac{P</em>\theta(w|c)}{P_\theta(w|c)+kP(w)}\right]+k\mathbb{E}<em>{P(w)}\left[\log\frac{kP(w)}{P</em>\theta(w|c)+kP(w)}\right] \right)\end{aligned}$$</p>
<p>最早提出NCE思想的论文<br><a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">Noise-Contrastive Estimation of Unnormalized Statistical Models-2010</a><br><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf">Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics-2012</a><br>给出了具体的NCE算法，本文主要参考来源于此<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1206.6426">A fast and simple algorithm for training neural probabilistic language models-2012</a></p>
<p>回顾一下分布的知识：<br>设真实数据概率分布的概率密度函数为 $P_d(\cdot)$ ，以下简称分布 $P_d(\cdot)$<br>机器学习的主要目标是 用一个参数为 $\theta$ 的分布 $P_\theta(\cdot)$ 估计  $P_d(\cdot)$，$P_\theta(\cdot)$称为预测概率分布</p>
<blockquote>
<p>如果能知道$P_\theta(\cdot)$的形式，比如是正态分布或指数分布，那么可以直接学习 $\theta$ 的值<br>但大部分情况下我们并不知道具体形式，所以是对每个给定数据的估计概率值，也就是直接学习概率分布</p>
</blockquote>
<p>概率分布要满足积分为1，即 $\int P(x)dx &#x3D; 1$</p>
<p>一般情况下，预测概率分布需要通过归一化，来保证满足积分为1的条件<br>$$P_\theta(\cdot)&#x3D;\frac{\hat{P}<em>\theta(\cdot)}{Z</em>\theta}$$<br>其中分子是非归一化的概率分布，分母 $Z_\theta$ 是配分函数（Partition Function）也称为归一化常数 （Normalized Constant） 或 Marginalized Evidence </p>
<p>用神经网络来估计为例<br>logits 层的输出 是非归一化的概率分布<br>经过softmax层之后才是 归一化的概率分布</p>
<h2 id="1-NCE-Noise-Contrastive-Estimation"><a href="#1-NCE-Noise-Contrastive-Estimation" class="headerlink" title="1. NCE: Noise Contrastive Estimation"></a>1. NCE: Noise Contrastive Estimation</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1206.6426">A fast and simple algorithm for training neural probabilistic language models</a><br>NCE 是一个机器学习的方法，不涉及神经网络</p>
<ul>
<li>学习一个参数来表示 $Z_\theta$</li>
<li>学习一个能区分 从真实数据分布和噪声分布采样数据的模型的模型</li>
</ul>
<p>假设我们的数据是文本，任务是根据给定的上下文context $c$，预测目标target为单词 $w$ ，希望学习到一个参数为$\theta$（用$\theta$参数化）的预测分布来估计&#x2F;建模真实分布：<br>$$P(w|c) \approx P_\theta(w|c)$$<br>让我们假设预测分布 $P_\theta$ 服从某一个指数族分布，任务是学习该分布的参数$\theta$ 值<br>$$P_\theta(w|c):&#x3D;:\frac{\exp{s_\theta(w,c)}}{\sum_{w\in V}\exp{s_\theta(w,c)}}:&#x3D;:\frac{u_\theta(w,c)}{Z_\theta}$$<br>$V$为词汇表，$S_θ(w,c)$是参数为 $θ$ 的评分函数，它量化了词$w$与上下文$c$的相容性，一般定义为向量点积</p>
<h3 id="1-1-ML-method"><a href="#1-1-ML-method" class="headerlink" title="1.1. ML method"></a>1.1. ML method</h3><p>在机器学习（ML）方法中，是通过最大似然估计（Maximum likelihood estimation,MLE）(假设所有样本之间相互独立)来优化参数 $\theta$，目标函数为最大化对数似然$\log P_\theta(w|c)$的期望：</p>
<p>$$\max_\theta L^c(\theta)&#x3D;\max_\theta\mathbb{E}<em>{w\sim P(w|c)}\left[\log P</em>\theta(w|c)\right]$$<br>这个期望展开为<br>$$<br>\mathbb{E}<em>{w\sim P(w|c)}\left[\log P</em>\theta(w|c)\right] &#x3D; \sum_{w \in V}P(w|c)\log P_\theta(w|c)<br>$$</p>
<p>对应的损失函数为  负对数似然$\log P_\theta(w|c)$的期望<br>$$\mathcal{L}<em>{MLE} &#x3D; -L^c(\theta)&#x3D;-\mathbb{E}</em>{w\sim P(w|c)}\left[\log P_\theta(w|c)\right] &#x3D; -\sum_{w \in V}P(w|c)\log P_\theta(w|c)$$<br>可以看到，这个其实就是类别数为 $|V|$ 的多分类交叉熵，</p>
<p>梯度为<br>$$\begin{aligned}\frac{\partial}{\partial\theta}L^c(\theta)&amp;&#x3D;\frac{\partial}{\partial\theta} \mathbb{E}<em>{w\sim P(w|c)}\left[\log P</em>{\theta}(w|c)\right]\&amp;&#x3D;\frac{\partial}{\partial\theta}\mathbb{E}<em>{w\sim P(w|c)}\left[\log\frac{\exp{s</em>\theta(w,c)}}{Z_\theta}\right]\<br>&amp;&#x3D;\frac{\partial}{\partial\theta}\mathbb{E}<em>{w\sim P(w|c)}s</em>\theta(w,c) - \frac{\partial}{\partial\theta} logZ_\theta\<br>&amp;&#x3D;\sum_{w\in V}[P(w|c)-P_\theta(w|c)]\frac{\partial}{\partial\theta}s_\theta(w,c)<br>\end{aligned}$$</p>
<p>实际计算中，给定一个在上下文 $c$ 中观察到的词 $w$，就对$L^c(\theta)$求一次梯度，P(w|c)只对观察到的词 $w$，为1：<br>$$<br>\begin{aligned}\frac{\partial}{\partial\theta}L^c(\theta)&amp;&#x3D;\sum_{w\in V}[P(w|c)-P_\theta(w|c)]\frac{\partial}{\partial\theta}s_\theta(w,c)\<br>&amp;&#x3D;\frac{\partial}{\partial\theta}s_\theta(w,c)-\sum_{w\in V}\frac{\exp s_\theta(w,c)}{\sum_{w\in V}\exp{s_\theta(w,c)}})\frac{\partial}{\partial\theta}s_\theta(w,c)<br>\end{aligned}<br>$$</p>
<p>优化他有些困难的，在计算梯度时计算词汇表中所有单词的$s_θ ( w , c)$来求 $P_\theta(w|c)$ 中的$Z_{\theta}$</p>
<p>论文里提到了Importance sampling 来解决 $Z_{\theta}$ 计算复杂度高的问题，但是存在一些缺点。</p>
<h3 id="1-2-NCE-method"><a href="#1-2-NCE-method" class="headerlink" title="1.2. NCE method"></a>1.2. NCE method</h3><p>噪声对比估计（Noise-Contrastive Estimation，NCE）:一种参数学习方法</p>
<p>不是通过最大似然估计直接求参数，而是通过对比来求参数，任务是学习一个能区分从真实数据分布和噪声分布采样数据的模型，从而学习到 $P_\theta(w|c)$</p>
<p>这个模型其实就是一个二元分类器 $P_\theta(D|w,c)$ ，来估计$P(D|w,c)$ ，标签D&#x3D;1或0分别表示 $w$ 是来自真实数据分布 $P(w|c)$ （论文中称为 $P^c_d$ ），还是噪声分布 $P(w)$ （论文中称为 $P_n$ ）</p>
<blockquote>
<p>二元分类器可以通过逻辑回归来进行学习。</p>
</blockquote>
<p>在噪声对比估计中，往往在数据分布 $P(w|c)$ 中采样1个正样本w，标签D&#x3D;1。然后从噪声分布 $P(w)$ 中采样k个负样本w，标签D&#x3D;0</p>
<p>也就是说，这k+1个样本构成的样本集$X$来自分布 $\frac{1}{k+1}P(w|c) + \frac{k}{k+1}P(w)$</p>
<p>那么标签D&#x3D;1，即样本来自真实分布 $P(w|c)$的后验概率为<br>$$P(D&#x3D;1|w,c)&#x3D;\frac{P(w|c)}{P(w|c)+kP(w)}$$</p>
<p>由于我们希望用$P_θ(w|c)$拟合$P(w|c)$，所以我们用$P_θ(w|c)$代替方程中的$P(w|c)$，使后验概率成参数$θ$的函数：<br>$$P_\theta(D&#x3D;1|w,c)&#x3D;\frac{P_\theta(w|c)}{P_\theta(w|c)+kP(w)}$$</p>
<p>我们简单地在真实数据和噪声样本的混合下得到的一个样本集$X$上做优化，最大化对数似然$log P_\theta( D | w , θ)$的期望值<br>$$\begin{aligned}\max_\theta J^c(\theta)&amp;&#x3D;\max_\theta \mathbb{E}<em>X[log P</em>\theta( D | w , θ)]<br>\&amp;&#x3D;\max_\theta \left(\mathbb{E}<em>{P(w|c)}\left[\log\frac{P</em>\theta(w|c)}{P_\theta(w|c)+kP(w)}\right]+k\mathbb{E}<em>{P(w)}\left[\log\frac{kP(w)}{P</em>\theta(w|c)+kP(w)}\right] \right)\end{aligned}$$</p>
<p>对$J^c(\theta)$ 求梯度<br>$$\begin{aligned}\frac{\partial}{\partial\theta}J^c(\theta)&amp;&#x3D; \frac{\partial} {\partial\theta}\left(\mathbb{E}<em>{P(w|c)}\left[\log\frac{P</em>\theta(w|c)}{P_\theta(w|c)+kP(w)}\right]+k\mathbb{E}<em>{P(w)}\left[\log\frac{kP(w)}{P</em>\theta(w|c)+kP(w)}\right] \right)<br>\<br>&amp;&#x3D;\mathbb{E}<em>{P(w|c)}\left[\frac{kP(w)}{P</em>\theta(w|c)+kP(w)}\frac{\partial} {\partial\theta}\log P_\theta(w|c)\right]-k\mathbb{E}<em>{P(w)}\left[\frac{P</em>\theta(w|c)}{P_\theta(w|c)+kP(w)}\frac{\partial} {\partial\theta}\log P_\theta(w|c)\right]\<br>&amp;&#x3D;\sum_{w\in V}(P(w|c)-P_\theta(w|c))\frac{kP(w)}{P_\theta(w|c)+kP(w)}\frac{\partial}{\partial\theta}\log P_\theta(w|c)\end{aligned}$$</p>
<p>当 $k → ∞$，趋近于最大似然的梯度<br>$$\frac{\partial}{\partial\theta}J^c(\theta)\to\sum_{w\in v}(P(w|c)-P_\theta(w|c))\frac{\partial}{\partial\theta}\log P_\theta(w|c)$$</p>
<p>实际训练过程中，给定一个在上下文$c$中观察到的词$w$，我们通过生成$k$个噪声样本$x_1,\dots,x_k$，$w$对梯度的贡献为<br>$$\begin{aligned}\frac{\partial}{\partial\theta}J^c(\theta)&#x3D;&amp;\frac{kP(w)}{P_{\theta}(w|c)+kP_{n}(w)}\frac{\partial}{\partial\theta}\operatorname{log}P_{\theta}(w|c)-\&amp;\begin{aligned}\sum_{i&#x3D;1}^k\left[\frac{P_\theta(x_i|c)}{P_\theta(x_i|c)+kP(x_i)}\frac{\partial}{\partial\theta}\log P_\theta(x_i|c)\right]\end{aligned}\end{aligned}$$</p>
<p>注意 $\frac{P_\theta(x_i|c)}{P_\theta(x_i|c)+kP(x_i)}$ 的值一定在0到1之间，不像importance sampling的方法一样会变得方差很大，基于NCE的学习是很稳定的</p>
<p>上文所述的 $J^c(\theta)$ 用于学习对某一个上下文$c$的分布$p(w|c)$，称为局部NCE目标函数</p>
<p>通过使用经验上下文概率P(c)作为权重来组合每个上下文c的NCE目标，定义全局NCE目标函数<br>$$J(\theta)&#x3D;\sum_cP(c)J(\theta)$$</p>
<h3 id="1-3-Dealing-with-normalizing-constants"><a href="#1-3-Dealing-with-normalizing-constants" class="headerlink" title="1.3. Dealing with normalizing constants"></a>1.3. Dealing with normalizing constants</h3><p>如上文所述， $P_\theta(w|c)$ 中的$Z_{\theta}$难以计算。NCE通过避免显式归一化和将$Z_{\theta}$作为要学习的参数处理这一问题。因此，模型被参数化为一个参数为 $\theta^0$ 非归一化分布$P_{θ^0}(w|c)$和一个参数$\phi$用于表示$Z_{\theta}$的对数<br>$$P_\theta(w|c)&#x3D;P_{\theta^0}(w|c)\exp(\phi)$$<br>那么 参数 $\theta &#x3D; {\theta^0,\phi}$<br>每对一个上下文 $c$ 都需要学习一个对应的 $\phi$，这使得难以扩展到具有大规模上下文的情况。</p>
<h4 id="1-3-1-negative-sampling-负采样"><a href="#1-3-1-negative-sampling-负采样" class="headerlink" title="1.3.1. negative sampling 负采样"></a>1.3.1. negative sampling 负采样</h4><p>论文发现将$Z_{\theta}$固定为1效果也很好，使用 $Z_{\theta}&#x3D;1$ 时的$J^c(\theta)$ 作为目标函数的方法为称为 负采样。</p>
<p>比如用负采样改进的了word2vec<br>$$\begin{aligned}P(D&#x3D;0\mid w,c)&amp;&#x3D;\frac{1}{u_\theta(w,c)+1}\P(D&#x3D;1\mid w,c)&amp;&#x3D;\frac{u_\theta(w,c)}{u_\theta(w,c)+1}.\end{aligned}$$<br>$u_\theta(w,c) &#x3D; \exp{s_\theta(w,c)}$</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1410.8251">Notes on Noise Contrastive Estimation and Negative Sampling</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/MarisaMagic/p/17949927">[NLP复习笔记] Word2Vec: 基于负采样的 Skip-gram 及其 SGD 训练 - 博客园</a></p>
<h3 id="1-4-复杂度"><a href="#1-4-复杂度" class="headerlink" title="1.4. 复杂度"></a>1.4. 复杂度</h3><p>假设$c$是上下文大小，$d$是单词特征向量维度，$V$是模型的词汇量大小。</p>
<p>利用公式计算预测表示，NCE和ML学习都需要进行$cd^2$操作。<br>对于ML，从预测的表示中计算下一个单词的分布大约需要$Vd$个操作。<br>对于NCE，在k个噪声样本下的分类为正样本的概率大约需要$kd$次操作<br>由于 k&lt;&lt;|V|，所以NCE大大提升了计算速度</p>
<h3 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5. 总结"></a>1.5. 总结</h3><p>总结一下，NCE做了两件事</p>
<ul>
<li>更改了目标函数，任务从多分类问题到二分类问题</li>
<li>验证了 $Z_{\theta}$ 在基于NCE的训练中可以直接设为1</li>
</ul>
<h2 id="2-扩展阅读，另一个博主的推导"><a href="#2-扩展阅读，另一个博主的推导" class="headerlink" title="2. 扩展阅读，另一个博主的推导"></a>2. 扩展阅读，另一个博主的推导</h2><p>感觉还是原论文里写的更精炼</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1yqcEeWEyc?spm_id_from=333.788.videopod.sections&vd_source=5b329c82286a01997454e14991ec6231">NCE噪声对比估计_哔哩哔哩_bilibili</a>中对NCE的推导：<br>在给定 $c$ 的情况下，正负样本的概率分别为<br>$$\begin{aligned}P(d&#x3D;1,w|c):&#x3D;P(w|d&#x3D;1,c)P(d&#x3D;1|c)&amp;&#x3D;P(w|d&#x3D;1,c)P(d&#x3D;1)\&amp;&#x3D;P(w|d&#x3D;1,c)\frac{1}{1+k}\end{aligned}$$<br>$$\begin{aligned}P(d&#x3D;0,w|c):&#x3D;P(w|d&#x3D;0,c)P(d&#x3D;0|c)&amp;&#x3D;P(w|d&#x3D;0,c)P(d&#x3D;0)\&amp;&#x3D;P(w|d&#x3D;0,c)\frac{k}{1+k}\end{aligned}$$<br>通过对 $d$ 求和，可以得到概率 $P(w|c)$<br>$$\begin{aligned}P(w|c)&#x3D;\sum_dP(d,w|c)&amp;&#x3D;P(d&#x3D;1,w|c)+P(d&#x3D;0,w|c)\&amp;&#x3D;P(w|d&#x3D;1,c)\frac{1}{1+k}+P(w|d&#x3D;0,c)\frac{k}{1+k}\end{aligned}$$</p>
<p>噪声对比估计的目标函数不再是最大化对数似然，而是<br>$$\max\left{\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log P</em>\theta(d&#x3D;1|w,c)\right]+k\mathbb{E}<em>{w\sim P(d&#x3D;0|w,c)}\left[\log P</em>\theta(d&#x3D;0|w,c)\right]\right}$$<br>P(w|d&#x3D;1,c)}其实就是 正样本的分布P_d  P(w|d&#x3D;0,c)}噪声分布 P(w|d&#x3D;0,c)} P_n</p>
<p>展开：<br>$$\begin{aligned}&amp;\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log P</em>\theta(d&#x3D;1|w,c)\right]+k\mathbb{E}<em>{w\sim P(w|d&#x3D;0,c)}\left[\log P</em>\theta(d&#x3D;0|w,c)\right]\<br>&amp;&#x3D;\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log\frac{P</em>\theta(w|d&#x3D;1,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}\right]+k\mathbb{E}<em>{w\sim P(w|d&#x3D;0,c)}\left[\log\frac{kP(w|d&#x3D;0,c)}{P</em>\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}\right]\<br>&amp;&#x3D; \sum_wP(w|d&#x3D;1,c)\frac{kP(w|d&#x3D;0,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)} \frac{\partial}{\partial\theta}\log P_\theta(w|d&#x3D;1,c)-\sum_wP(w|d&#x3D;0,c)\frac{kP_\theta(w|d&#x3D;1,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}\frac{\partial}{\partial\theta}\log P_\theta(w|d&#x3D;1,c)<br>\end{aligned}$$<br>可以证明：<br>当 $k\rightarrow \infty$ 时，并把 $logZ_\theta(c)$ 当做常数 ，有<br>$$\begin{aligned}&amp;\frac{\partial}{\partial\theta}\left[\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log P</em>\theta(d&#x3D;1|w,c)\right]+k\mathbb{E}<em>{w\sim P(w|d&#x3D;0,c)}\left[\log P</em>\theta(d&#x3D;0|w,c)\right]\right]\&amp;&#x3D;\sum_w\left[P(w|d&#x3D;1,c)-P_\theta(w|d&#x3D;1,c)\right]\frac{\partial}{\partial\theta}s_\theta(w,c)\end{aligned}$$<br>可以发现：<br>在这个情况下，最大化噪声对比估计 等价与最大化似然</p>
<p>我们可以用蒙特卡洛采样法去近似期望，即从数据分布中采样m个点，然后从噪声分布中采样n个点<br>$$&#x3D;\frac{1}{m}\sum_{w}\log\frac{P_{\theta}(w|d&#x3D;1,c)}{P_{\theta}(w,|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}+\frac{k}{n}\sum_{w^{-}}\log\frac{kP(w^-|d&#x3D;0,c)}{P_{\theta}(w^-|d&#x3D;1,c)+kP(w^-|d&#x3D;0,c)}$$</p>
<p>当m&#x3D;1,n&#x3D;k。那么为<br>$$\log\frac{P_\theta(w|d&#x3D;1,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}+\sum_{w_-}\log\frac{kP(w|d&#x3D;0,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}$$</p>
<p>负采样是NCE的一种特殊情况，即让归一化项$Z_{\theta}$固定为常数1且令$kP(w|d&#x3D;0,c)&#x3D;1\to P(w|d&#x3D;0,c)&#x3D;\frac1k$,</p>
<p>那么<br>$$\begin{aligned}&amp;\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log P</em>\theta(d&#x3D;1|w,c)\right]+k\mathbb{E}<em>{w\sim P(w|d&#x3D;0,c)}\left[\log P</em>\theta(d&#x3D;0|w,c)\right]\<br>&amp;&#x3D;\frac{1}{m}\sum_{w}\log\frac{\exp{s_\theta(w,c)}}{\exp{s_\theta(w,c)}+1}+\frac{k}{n}\sum_{w_-}\log\frac{1}{\exp{s_\theta(w,c)}+1}\<br>&amp;&#x3D;\frac{1}{m}\sum_{w}\log\frac{\exp{s_{\theta}(w,c)}&#x2F;\exp{s_{\theta}(w,c)}}{(\exp{s_{\theta}(w,c)}+1)&#x2F;\exp{s_{\theta}(w,c)}}+\frac{k}{n}\sum_{w}\log\frac{1}{\exp{s_{\theta}(w,c)}+1}\<br>&amp;&#x3D;\frac{1}{m}\sum_{w}\log\frac{1}{1+\exp{-s_{\theta}(w,c)}}+\frac{k}{n}\sum_{w}\log\frac{1}{\exp{s_{\theta}(w,c)}+1}\<br>&amp;&#x3D;\frac{1}{m}\sum_{w}\log\sigma(s_{\theta}(w,c))+\frac{k}{n}\sum_{w}\log\sigma(-s_{\theta}(w,c))<br>\end{aligned}$$</p>
<p>当m&#x3D;1,n&#x3D;k,则<br>$$&#x3D;\log\sigma(s_\theta(w,c))+\sum\log\sigma(-s_\theta(w,c))$$</p>
<p>loss<br>$$<br>-\log\sigma(s_\theta(w,c))-\sum\log\sigma(-s_\theta(w,c))<br>$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E5%99%AA%E5%A3%B0%E5%AF%B9%E6%AF%94%E4%BC%B0%E8%AE%A1-NCE/" data-id="cmeqis6j70000ucv1aeif4q86" data-title="噪声对比估计-NCE" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          损失函数-InfoNCE
        
      </div>
    </a>
  
  
    <a href="/2025/08/11/web/framework/category%20test/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">category test</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/web/">web</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/web/framework/">framework</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">3.对比学习</a></li></ul></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/08/">August 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/">损失函数-InfoNCE</a>
          </li>
        
          <li>
            <a href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E5%99%AA%E5%A3%B0%E5%AF%B9%E6%AF%94%E4%BC%B0%E8%AE%A1-NCE/">噪声对比估计-NCE</a>
          </li>
        
          <li>
            <a href="/2025/08/11/web/framework/category%20test/">category test</a>
          </li>
        
          <li>
            <a href="/2025/08/11/web/framework/husky%20test/">husky test</a>
          </li>
        
          <li>
            <a href="/2025/08/07/front%20matter%20test/">front matter test</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>