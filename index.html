<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-机器学习/3.对比学习/损失函数-InfoNCE" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/" class="article-date">
  <time class="dt-published" datetime="2025-08-25T02:09:20.000Z" itemprop="datePublished">2025-08-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>►<a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">3.对比学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/">损失函数-InfoNCE</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><img src="/resources/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/img-20250825100450172.png"></p>
<p>机器学习领域，尤其是<strong>无监督学习和表示学习</strong>中，对比学习（Contrastive Learning）已经成为一种非常流行的方法。<strong>通过最大化与正样本的相似性，同时最小化与负样本的相似性</strong>，使得训练模型能区分“相关”和“不相关”的数据对，从而捕获数据的深层语义信息。</p>
<p>其中，InfoNCE Loss 是一种广泛使用的损失函数。<br>InfoNCE loss ：<br>$$\mathcal{L}_N &#x3D; -\mathbb{E}<em>X \left[ \log \frac{f_k(x_i, c_t)}{\sum</em>{x_j \in X} f_k(x_j, c_t)} \right] \tag{4}$$</p>
<p>InfoNCE 全称是 Info Noise-Contrastive Estimation Loss，基于噪声对比估计（Noise-Contrastive Estimation, NCE）。</p>
<p>在InfoNCE Loss的背后，<br>首次提出：CPC[Contrastive Predictive Coding]<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.03748">[1807.03748] Representation Learning with Contrastive Predictive Coding</a><br>应用：对比学习，大模型训练如 CLIP[Contrastive Language-Image Pretraining]所采用。<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.00020">[2103.00020] Learning Transferable Visual Models From Natural Language Supervision</a></p>
<h2 id="1-CPC"><a href="#1-CPC" class="headerlink" title="1. CPC"></a>1. CPC</h2><p>CPC简介:来着google DeepMind 2019<br>[CPC-Representation Learning with Contrastive Predictive Coding]：基于对比预测编码的表示学习<br>PPT：<a target="_blank" rel="noopener" href="https://www2.cs.arizona.edu/~pachecoj/courses/csc696h_spring24/lectures/thang_cpc.pdf">Representation Learning with Contrastive Predictive Coding</a></p>
<ol>
<li>CPC是一个unsupervised representation learning 方法。比起有监督学习，更能学到不针对单个有监督任务特化的特征（即表示，representation）</li>
<li>它可以用于序列数据(文本、语音信号等)，也可以用于图片和强化学习</li>
</ol>
<p>在无监督的情况下，如何定义训练目标（定义表示的好坏）？<br>最常见的思路是预测编码（predictive coding），即学到的表示要能够用来 预测未来（future） 或 预测缺失词（missing） 或 预测上下文（context）。比如词嵌入模型Word2Vec中的CBOW和Skip-gram，分别对应后两个预测目标。</p>
<p>CPC 假设预测编码方法的有效性来自于：预测目标值的上下文通常有条件地依赖于相同的共享的高层潜在信息。并且通过将其作为一个预测问题，能自动推断包含潜在信息的特征来进行表示学习。</p>
<p>CPC希望学习到的这个表示能预测未来。</p>
<p>设当前的上下文为 $c$ ，预测未来目标为 $x$， 如果用生成模型来建模 $p(x|c$)（条件概率分布） 在高维数据中非常困难，因为它需要生成数据的每一个细节。而且单模态损失如均方误差和交叉熵并不是很有用。</p>
<p>CPC的做法是：<strong>让 $c$ 和 $x$ 之间的表示保留尽可能多的互信息 (Mutual Information, MI)</strong>。这样的表示能编码高维输入信号不同部分之间的潜在共享信息（latent shared information），并且丢弃低维信息和局部噪声</p>
<p>$x$ 和 $c$ 的互信息定义为<br>$$I(x;c)&#x3D;\sum_{x,c}p(x,c)\log\frac{p(x|c)}{p(x)} \tag{1}$$</p>
<blockquote>
<p>互信息（Mutual Information）：指变量间的相关性，通常用I(X;Y)表示X和Y之间的互信息，表示引入Y后使X的不确定度减小的量，I(X;Y)越大可以说明两者关系越密切</p>
</blockquote>
<p><img src="/resources/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/img-20250825100546253.png"></p>
<p>在这个图里，raw data是最下面的<strong>语音信号</strong>，在这条语音信号上选取一些时间窗口（frames），每一个frame作为输入$x$，构成序列 ${x_t}$，</p>
<p>CPC用一个encoder $g_{enc}$（比如AutoEncoder或者CNN），对每个 $x_t$ 编码得到 latent vector $z_t &#x3D; g_{enc}(x_t)$** ，为了做预测，把序列${ z_t}$放到一个可以做预测的，有回归性质的模型 $g_{ar}$ 里（比如RNN），用 $t$ 及其之前的frames为输入 ${z_{\leq t}}$ ，得到 $c_t&#x3D;g_{ar}(z_{\leq t})$ </p>
<p>按上节所说，CPC的巧妙之处在于，它不直接建模 ($p(x_{t+k}|c_t)$)，而是用一个评分函数 $f_k(x_{t+k}, c_t)$ <strong>建模数据的条件分布与独立分布之间的密度比</strong>，<br>$$f_k(x_{t+k}, c_t) \propto \frac{p(x_{t+k}|c_t)}{p(x_{t+k})} \tag{2}$$<br>右项的密度比来自互信息方程(1)，衡量的是$x_{t+k}$在给定 $c_t$ 的条件下出现的可能性，相比它独立出现的可能性。如果 $x_{t+k}$ 和 $c_t$ 高度相关，这个比值会很大；如果不相关，则接近 1 或更小。</p>
<p>左项评分函数$f_k(x_{t+k}, c_t)$计算为，<br>$$f_k(x_{t+k},c_t)&#x3D;exp(z_{t+k}^TW_kc_t) \tag{3}$$<br>直接用线性矩阵 $W_1,W_2,\dots,W_k$ 乘以 $c_t$ 做预测（也可以用RNN做）得到 $\hat{z}<em>{t+k} &#x3D; W_kc_t$，然后用向量内积来衡量 $\hat{z}</em>{t+k}$ 和$z_{t+k}$的相似度。</p>
<p>现在问题来到怎么训练使评分函数 $f_k()$真的能估计密度比呢？ **CPC设计了基于NCE的 InfoNCE Loss 如下：<br>$$\mathcal{L}<em>N &#x3D; -\mathbb{E}<em>X \left[ \log \frac{f_k(x</em>{t+k}, c_t)}{\sum</em>{x_j \in X} f_k(x_j, c_t)} \right] \tag{4}$$</p>
<ul>
<li>$X &#x3D; {x_1, \ldots, x_N}$是一个样本集，包含<ul>
<li>1个正样本(positive sample)，与上下文 $c_t$ 相关，采样自 $p(x_{t+k} | c_t)$，即正在用的那条语音信号K步之内的frame $x_{t+k}$ </li>
<li>$N - 1$ 个负样本(negative&#x2F;noise sample），与上下文 $c_t$ 无关，采样自 $p(x_{t+k})$，即K步之外的frame或从其他条的语音信号里随机选择的一个frame $x_j$</li>
</ul>
</li>
<li>$f_k(x_{t+k}, c_t)$ 是一个评分函数，表示正样本对 $(x_{t+k}, c_t)$ 的匹配程度。$\sum_{x_j \in X} f_k(x_j, c_t)$ 是正样本和所有负样本评分的总和。</li>
</ul>
<p>$g_{enc}$和$g_{ar}$还有线性矩阵都进行联合训练以最小化InfoNCE loss，<br><strong>$z_t$和$c_t$均可作为表示。当过去的信息有用时$c_t$  ，当不需要额外上下文信息时$z_t$。</strong></p>
<p>直观来看，最小化InfoNCE loss将最大化正样本的评分 $f_k(x_{t+k}, c_t)$ 相对于所有样本评分之和的比例，实际上是在<strong>训练模型识别“真正相关的样本对”</strong>，使 $c_t$ 的预测和正样本 $x_{t+k}$ 的表示相似（接近）。但如何解释InfoNSE真的能使评分函数 $f_k()$估计密度比呢？</p>
<h3 id="1-1-InfoNCE背后的原理"><a href="#1-1-InfoNCE背后的原理" class="headerlink" title="1.1. InfoNCE背后的原理"></a>1.1. InfoNCE背后的原理</h3><p>如果能证明InfoNCE真的能使评分函数 $f_k()$估计密度比。那么最大化正样本的评分 $f_k(x_{t+k}, c_t)$ 就能最大化密度比$\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$</p>
<p>证明：<br>InfoNCE loss ：<br>$$\mathcal{L}_N &#x3D; -\mathbb{E}<em>X \left[ \log \frac{f_k(x_i, c_t)}{\sum</em>{x_j \in X} f_k(x_j, c_t)} \right] \tag{4}$$</p>
<p>InfoNCE loss在形式上是<strong>分类交叉熵</strong>，<br>$\frac{f_k}{\sum_Xf_k}$是 第$i$个样本$x_i$ 类别为正样本的预测概率，以下改写为 $p( d &#x3D; i | X , c_t)$。</p>
<p>最小化InfoNCE loss等价于最大化预测 $x_i$ 类别为正样本的概率</p>
<p>回忆一下，我们是构造了一组随机样本$X&#x3D;{x_{1},\cdots,x_{N}}$，里面有一个正样本$x_{i}$ ，采样自$x_{i}\sim p(x|c)$。而其余的是负样本$x_{l\neq i}$，采样自$p(x)$</p>
<p>$p( d &#x3D; i | X , c_t)$可以计算为<br>$$\begin{gathered}p(d&#x3D;i|X,c_{t})&#x3D;\frac{p(x_i|c_t)\prod_{l\neq i}p(x_l)}{\sum_{j&#x3D;1}^N [p(x_j|c_t)\prod_{l\neq j}p(x_l)]}\&#x3D;\frac{\frac{p(x_i|c_t)}{p(x_i)}}{\sum_{j&#x3D;1}^N\frac{p(x_j|c_t)}{p(x_j)}}.\end{gathered} \tag{5}$$</p>
<p>从上式可以证明，式(4)中$f_k(x_{t+k}, c_t)$ 与密度比$\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$成正比，与负样本个数$N - 1$的选择无关。</p>
<p>也就是说 最小化InfoNCE loss等价于最大化预测 $x_i$ 类别为正样本的概率，等价最大化了密度比$\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$</p>
<p>附录证明了最小化InfoNCE loss，不仅最大化密度比$\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$，也确实最大化 $x_{t+k}$和 $c_t$ 之间的互信息的下限<br>$$I(x_{t+k},c_{t})\geq\log(N)-\mathcal{L}_{\mathrm{N}}$$<br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1BhfzYwEUi?spm_id_from=333.788.videopod.sections&vd_source=5b329c82286a01997454e14991ec6231">InfoNCE：互信息噪声对比估计_哔哩哔哩_bilibili</a></p>
<h2 id="2-Experiment"><a href="#2-Experiment" class="headerlink" title="2. Experiment"></a>2. Experiment</h2><p><strong>强调：CPC学到的是表示，能预测的也是表示</strong></p>
<p>CPC论文里做了语音信号，视觉、自然语言和强化学习的实验</p>
<h3 id="2-1-Audio"><a href="#2-1-Audio" class="headerlink" title="2.1. Audio"></a>2.1. Audio</h3><p>使用公开的LibriSpeech英语语音数据集的100小时子集[30]。该数据集只提供原始文本，没有额外的标签。该数据集包含了251个不同speaker的语音。每10ms作为一个frame，通过Kaldi工具包[31]和在Librispeech上预训练的模型获得了对齐的phone标签。在长度为20480的采样音频窗口上进行训练。</p>
<blockquote>
<ul>
<li><strong>phoneme</strong>（音位）是语音学中最小的有区别性的单位，表示在某种语言中具有区分意义的音。</li>
</ul>
</blockquote>
<ul>
<li><strong>phone</strong>（音素）是phoneme的具体实现形式，指的是实际发出的声音。<br>  简单来说，phoneme是一个抽象的概念，而phone是其具体的发音表现形式。</li>
</ul>
<p>预测语音信号未来1-20个frame的latent vector $z$ 的平均准确率。<br><img src="/resources/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/img-20250825100653204.png"></p>
<p>对$g_{ar}$的输出$c_t$ (256维)，使用线性逻辑回归分类器分类。phone分类和speaker分类的准确性。</p>
<p><strong>梅尔频率倒谱系数（MFCC）</strong> 是语音识别中广泛使用的一种特征提取方法。</p>
<p><img src="/resources/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/img-20250825100708369.png"></p>
<p>两项针对phone分类的CPC消融研究。</p>
<ul>
<li>改变了预测步数，这表明预测多步对于学习有用的表示是重要的</li>
<li>固定预测步数为12，<ul>
<li>mixed speaker，负样本包含不同speaker的语音信号</li>
<li>same speaker：与相同说话人实验(第二行)相反。</li>
<li>在第三个和第四个实验中，排除当前语音信号，从(因此, X中只存在小批量数据中的其他例子)中提取负样本，</li>
<li>在最后一个实验中，只提取序列(因此所有样本均来自同一说话人)中的负样本。<br><img src="/resources/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/img-20250825100724807.png"></li>
</ul>
</li>
</ul>
<h3 id="2-2-Natural-Language"><a href="#2-2-Natural-Language" class="headerlink" title="2.2. Natural Language"></a>2.2. Natural Language</h3><p>做的是transfer learning实验，严格遵循了Skip-thought[ 26 ]的步骤。首先在BookCorpus数据集[42]上学习无监督模型，在一组新数据集上做句子（sentence）分类任务。为了处理在训练过程中没有看到的单词，采用与Skip-thought相同的方法进行词扩展，即在word2vec和模型学习到的词嵌入之间构建一个线性映射。</p>
<p>电影评论情感(MR) [43]，客户产品评论(CR) [44]，主客二分(subj)[45]，观点极性(MPQA) [46]和问题类型分类(TREC) [47]。</p>
<p>Paragraph-vector 无监督的句子级表示学习方法。<br>Skip-thought[26]在Word2Vec的基础上使用LSTM做单词预测，并使用最大似然对观测序列进行预测。Skip-thought LM 是加了Layer Norm。就是上文说的用生成模型来做预测，相对于CPC来说更难训练得多。</p>
<p><img src="/resources/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/img-20250825100739168.png"></p>
<h3 id="2-3-vision"><a href="#2-3-vision" class="headerlink" title="2.3. vision"></a>2.3. vision</h3><p>训练过程如下：从一幅256 × 256的图像中提取一个由64 × 64 的patch组成的7 × 7网格，重叠32个像素。</p>
<p>然后通过ResNet-v2-101编码器对每个patch进行编码。使用类似 pixelCNN 的自回归模型将其转化成一个序列类型，用前几个 patch 作为输入，预测之后的 patch。<br><img src="/resources/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/img-20250825100749409.png"></p>
<p>ImageNet top-1非监督分类结果<br>计算机视觉中，常对跟踪到的视频块使用三元组损失（Triplet loss），使得来自同一对象在不同时间步的块比随机块更相似。[11、29]提出预测图像中块的相对位置，在[10]中颜色值是从灰度图像中预测的。</p>
<p>&#x3D;&#x3D;啥是三元组损失&#x3D;&#x3D; [FaceNet：A Unified Embedding for Face Recognition],参阅 <a target="_blank" rel="noopener" href="https://blog.csdn.net/zenglaoshi/article/details/106928204">深度学习之三元组损失原理与选取策略_三元组损失函数效果特别差-CSDN博客</a></p>
<p><img src="/resources/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/img-20250825100802365.png"></p>
<h3 id="2-4-Reinforcement-Learning"><a href="#2-4-Reinforcement-Learning" class="headerlink" title="2.4. Reinforcement Learning"></a>2.4. Reinforcement Learning</h3><p>在DeepMind Lab [51]的3D环境中评估了所提出的无监督学习方法在五种强化学习中的表现：room _ watermaze，explore _ goal _ location _ small，searchvoid _ arena _ 01，lasertag _ three _ opposites _ small和room _ key_doors_puzzle。<br>以批量A2C [52] agent为基本模型，并添加CPC作为辅助损失，使学习到的表征编码了关于未来观测的分布。不使用重放replay buffer，因此预测结果必须适应策略的变化行为。</p>
<p><img src="/resources/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/img-20250825100815646.png"></p>
<p>黑色：批量A2C基线，红色：辅助对比丢失</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shizheng_Li/article/details/146709102?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-146709102-blog-134539003.235%5Ev43%5Epc_blog_bottom_relevance_base2&spm=1001.2101.3001.4242.2&utm_relevant_index=4">深入解析 InfoNCE Loss：对比学习的基石-CSDN博客</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/shizheng_Li/article/details/146710000?spm=1001.2014.3001.5501">什么是互信息（Mutual Information, MI）？CSDN博客</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/129076690">理解Contrastive Predictive Coding和NCE Loss - 知乎</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/" data-id="cmeqis6jc0001ucv189o5fu9w" data-title="损失函数-InfoNCE" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习/3.对比学习/噪声对比估计-NCE" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E5%99%AA%E5%A3%B0%E5%AF%B9%E6%AF%94%E4%BC%B0%E8%AE%A1-NCE/" class="article-date">
  <time class="dt-published" datetime="2025-08-25T02:08:52.000Z" itemprop="datePublished">2025-08-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>►<a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">3.对比学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E5%99%AA%E5%A3%B0%E5%AF%B9%E6%AF%94%E4%BC%B0%E8%AE%A1-NCE/">噪声对比估计-NCE</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>NCE目标函数：<br>$$\begin{aligned}\max_\theta J^c(\theta)&amp;&#x3D;\max_\theta \mathbb{E}<em>X[log P</em>\theta( D | w , θ)]<br>\&amp;&#x3D;\max_\theta \left(\mathbb{E}<em>{P(w|c)}\left[\log\frac{P</em>\theta(w|c)}{P_\theta(w|c)+kP(w)}\right]+k\mathbb{E}<em>{P(w)}\left[\log\frac{kP(w)}{P</em>\theta(w|c)+kP(w)}\right] \right)\end{aligned}$$</p>
<p>最早提出NCE思想的论文<br><a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">Noise-Contrastive Estimation of Unnormalized Statistical Models-2010</a><br><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf">Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics-2012</a><br>给出了具体的NCE算法，本文主要参考来源于此<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1206.6426">A fast and simple algorithm for training neural probabilistic language models-2012</a></p>
<p>回顾一下分布的知识：<br>设真实数据概率分布的概率密度函数为 $P_d(\cdot)$ ，以下简称分布 $P_d(\cdot)$<br>机器学习的主要目标是 用一个参数为 $\theta$ 的分布 $P_\theta(\cdot)$ 估计  $P_d(\cdot)$，$P_\theta(\cdot)$称为预测概率分布</p>
<blockquote>
<p>如果能知道$P_\theta(\cdot)$的形式，比如是正态分布或指数分布，那么可以直接学习 $\theta$ 的值<br>但大部分情况下我们并不知道具体形式，所以是对每个给定数据的估计概率值，也就是直接学习概率分布</p>
</blockquote>
<p>概率分布要满足积分为1，即 $\int P(x)dx &#x3D; 1$</p>
<p>一般情况下，预测概率分布需要通过归一化，来保证满足积分为1的条件<br>$$P_\theta(\cdot)&#x3D;\frac{\hat{P}<em>\theta(\cdot)}{Z</em>\theta}$$<br>其中分子是非归一化的概率分布，分母 $Z_\theta$ 是配分函数（Partition Function）也称为归一化常数 （Normalized Constant） 或 Marginalized Evidence </p>
<p>用神经网络来估计为例<br>logits 层的输出 是非归一化的概率分布<br>经过softmax层之后才是 归一化的概率分布</p>
<h2 id="1-NCE-Noise-Contrastive-Estimation"><a href="#1-NCE-Noise-Contrastive-Estimation" class="headerlink" title="1. NCE: Noise Contrastive Estimation"></a>1. NCE: Noise Contrastive Estimation</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1206.6426">A fast and simple algorithm for training neural probabilistic language models</a><br>NCE 是一个机器学习的方法，不涉及神经网络</p>
<ul>
<li>学习一个参数来表示 $Z_\theta$</li>
<li>学习一个能区分 从真实数据分布和噪声分布采样数据的模型的模型</li>
</ul>
<p>假设我们的数据是文本，任务是根据给定的上下文context $c$，预测目标target为单词 $w$ ，希望学习到一个参数为$\theta$（用$\theta$参数化）的预测分布来估计&#x2F;建模真实分布：<br>$$P(w|c) \approx P_\theta(w|c)$$<br>让我们假设预测分布 $P_\theta$ 服从某一个指数族分布，任务是学习该分布的参数$\theta$ 值<br>$$P_\theta(w|c):&#x3D;:\frac{\exp{s_\theta(w,c)}}{\sum_{w\in V}\exp{s_\theta(w,c)}}:&#x3D;:\frac{u_\theta(w,c)}{Z_\theta}$$<br>$V$为词汇表，$S_θ(w,c)$是参数为 $θ$ 的评分函数，它量化了词$w$与上下文$c$的相容性，一般定义为向量点积</p>
<h3 id="1-1-ML-method"><a href="#1-1-ML-method" class="headerlink" title="1.1. ML method"></a>1.1. ML method</h3><p>在机器学习（ML）方法中，是通过最大似然估计（Maximum likelihood estimation,MLE）(假设所有样本之间相互独立)来优化参数 $\theta$，目标函数为最大化对数似然$\log P_\theta(w|c)$的期望：</p>
<p>$$\max_\theta L^c(\theta)&#x3D;\max_\theta\mathbb{E}<em>{w\sim P(w|c)}\left[\log P</em>\theta(w|c)\right]$$<br>这个期望展开为<br>$$<br>\mathbb{E}<em>{w\sim P(w|c)}\left[\log P</em>\theta(w|c)\right] &#x3D; \sum_{w \in V}P(w|c)\log P_\theta(w|c)<br>$$</p>
<p>对应的损失函数为  负对数似然$\log P_\theta(w|c)$的期望<br>$$\mathcal{L}<em>{MLE} &#x3D; -L^c(\theta)&#x3D;-\mathbb{E}</em>{w\sim P(w|c)}\left[\log P_\theta(w|c)\right] &#x3D; -\sum_{w \in V}P(w|c)\log P_\theta(w|c)$$<br>可以看到，这个其实就是类别数为 $|V|$ 的多分类交叉熵，</p>
<p>梯度为<br>$$\begin{aligned}\frac{\partial}{\partial\theta}L^c(\theta)&amp;&#x3D;\frac{\partial}{\partial\theta} \mathbb{E}<em>{w\sim P(w|c)}\left[\log P</em>{\theta}(w|c)\right]\&amp;&#x3D;\frac{\partial}{\partial\theta}\mathbb{E}<em>{w\sim P(w|c)}\left[\log\frac{\exp{s</em>\theta(w,c)}}{Z_\theta}\right]\<br>&amp;&#x3D;\frac{\partial}{\partial\theta}\mathbb{E}<em>{w\sim P(w|c)}s</em>\theta(w,c) - \frac{\partial}{\partial\theta} logZ_\theta\<br>&amp;&#x3D;\sum_{w\in V}[P(w|c)-P_\theta(w|c)]\frac{\partial}{\partial\theta}s_\theta(w,c)<br>\end{aligned}$$</p>
<p>实际计算中，给定一个在上下文 $c$ 中观察到的词 $w$，就对$L^c(\theta)$求一次梯度，P(w|c)只对观察到的词 $w$，为1：<br>$$<br>\begin{aligned}\frac{\partial}{\partial\theta}L^c(\theta)&amp;&#x3D;\sum_{w\in V}[P(w|c)-P_\theta(w|c)]\frac{\partial}{\partial\theta}s_\theta(w,c)\<br>&amp;&#x3D;\frac{\partial}{\partial\theta}s_\theta(w,c)-\sum_{w\in V}\frac{\exp s_\theta(w,c)}{\sum_{w\in V}\exp{s_\theta(w,c)}})\frac{\partial}{\partial\theta}s_\theta(w,c)<br>\end{aligned}<br>$$</p>
<p>优化他有些困难的，在计算梯度时计算词汇表中所有单词的$s_θ ( w , c)$来求 $P_\theta(w|c)$ 中的$Z_{\theta}$</p>
<p>论文里提到了Importance sampling 来解决 $Z_{\theta}$ 计算复杂度高的问题，但是存在一些缺点。</p>
<h3 id="1-2-NCE-method"><a href="#1-2-NCE-method" class="headerlink" title="1.2. NCE method"></a>1.2. NCE method</h3><p>噪声对比估计（Noise-Contrastive Estimation，NCE）:一种参数学习方法</p>
<p>不是通过最大似然估计直接求参数，而是通过对比来求参数，任务是学习一个能区分从真实数据分布和噪声分布采样数据的模型，从而学习到 $P_\theta(w|c)$</p>
<p>这个模型其实就是一个二元分类器 $P_\theta(D|w,c)$ ，来估计$P(D|w,c)$ ，标签D&#x3D;1或0分别表示 $w$ 是来自真实数据分布 $P(w|c)$ （论文中称为 $P^c_d$ ），还是噪声分布 $P(w)$ （论文中称为 $P_n$ ）</p>
<blockquote>
<p>二元分类器可以通过逻辑回归来进行学习。</p>
</blockquote>
<p>在噪声对比估计中，往往在数据分布 $P(w|c)$ 中采样1个正样本w，标签D&#x3D;1。然后从噪声分布 $P(w)$ 中采样k个负样本w，标签D&#x3D;0</p>
<p>也就是说，这k+1个样本构成的样本集$X$来自分布 $\frac{1}{k+1}P(w|c) + \frac{k}{k+1}P(w)$</p>
<p>那么标签D&#x3D;1，即样本来自真实分布 $P(w|c)$的后验概率为<br>$$P(D&#x3D;1|w,c)&#x3D;\frac{P(w|c)}{P(w|c)+kP(w)}$$</p>
<p>由于我们希望用$P_θ(w|c)$拟合$P(w|c)$，所以我们用$P_θ(w|c)$代替方程中的$P(w|c)$，使后验概率成参数$θ$的函数：<br>$$P_\theta(D&#x3D;1|w,c)&#x3D;\frac{P_\theta(w|c)}{P_\theta(w|c)+kP(w)}$$</p>
<p>我们简单地在真实数据和噪声样本的混合下得到的一个样本集$X$上做优化，最大化对数似然$log P_\theta( D | w , θ)$的期望值<br>$$\begin{aligned}\max_\theta J^c(\theta)&amp;&#x3D;\max_\theta \mathbb{E}<em>X[log P</em>\theta( D | w , θ)]<br>\&amp;&#x3D;\max_\theta \left(\mathbb{E}<em>{P(w|c)}\left[\log\frac{P</em>\theta(w|c)}{P_\theta(w|c)+kP(w)}\right]+k\mathbb{E}<em>{P(w)}\left[\log\frac{kP(w)}{P</em>\theta(w|c)+kP(w)}\right] \right)\end{aligned}$$</p>
<p>对$J^c(\theta)$ 求梯度<br>$$\begin{aligned}\frac{\partial}{\partial\theta}J^c(\theta)&amp;&#x3D; \frac{\partial} {\partial\theta}\left(\mathbb{E}<em>{P(w|c)}\left[\log\frac{P</em>\theta(w|c)}{P_\theta(w|c)+kP(w)}\right]+k\mathbb{E}<em>{P(w)}\left[\log\frac{kP(w)}{P</em>\theta(w|c)+kP(w)}\right] \right)<br>\<br>&amp;&#x3D;\mathbb{E}<em>{P(w|c)}\left[\frac{kP(w)}{P</em>\theta(w|c)+kP(w)}\frac{\partial} {\partial\theta}\log P_\theta(w|c)\right]-k\mathbb{E}<em>{P(w)}\left[\frac{P</em>\theta(w|c)}{P_\theta(w|c)+kP(w)}\frac{\partial} {\partial\theta}\log P_\theta(w|c)\right]\<br>&amp;&#x3D;\sum_{w\in V}(P(w|c)-P_\theta(w|c))\frac{kP(w)}{P_\theta(w|c)+kP(w)}\frac{\partial}{\partial\theta}\log P_\theta(w|c)\end{aligned}$$</p>
<p>当 $k → ∞$，趋近于最大似然的梯度<br>$$\frac{\partial}{\partial\theta}J^c(\theta)\to\sum_{w\in v}(P(w|c)-P_\theta(w|c))\frac{\partial}{\partial\theta}\log P_\theta(w|c)$$</p>
<p>实际训练过程中，给定一个在上下文$c$中观察到的词$w$，我们通过生成$k$个噪声样本$x_1,\dots,x_k$，$w$对梯度的贡献为<br>$$\begin{aligned}\frac{\partial}{\partial\theta}J^c(\theta)&#x3D;&amp;\frac{kP(w)}{P_{\theta}(w|c)+kP_{n}(w)}\frac{\partial}{\partial\theta}\operatorname{log}P_{\theta}(w|c)-\&amp;\begin{aligned}\sum_{i&#x3D;1}^k\left[\frac{P_\theta(x_i|c)}{P_\theta(x_i|c)+kP(x_i)}\frac{\partial}{\partial\theta}\log P_\theta(x_i|c)\right]\end{aligned}\end{aligned}$$</p>
<p>注意 $\frac{P_\theta(x_i|c)}{P_\theta(x_i|c)+kP(x_i)}$ 的值一定在0到1之间，不像importance sampling的方法一样会变得方差很大，基于NCE的学习是很稳定的</p>
<p>上文所述的 $J^c(\theta)$ 用于学习对某一个上下文$c$的分布$p(w|c)$，称为局部NCE目标函数</p>
<p>通过使用经验上下文概率P(c)作为权重来组合每个上下文c的NCE目标，定义全局NCE目标函数<br>$$J(\theta)&#x3D;\sum_cP(c)J(\theta)$$</p>
<h3 id="1-3-Dealing-with-normalizing-constants"><a href="#1-3-Dealing-with-normalizing-constants" class="headerlink" title="1.3. Dealing with normalizing constants"></a>1.3. Dealing with normalizing constants</h3><p>如上文所述， $P_\theta(w|c)$ 中的$Z_{\theta}$难以计算。NCE通过避免显式归一化和将$Z_{\theta}$作为要学习的参数处理这一问题。因此，模型被参数化为一个参数为 $\theta^0$ 非归一化分布$P_{θ^0}(w|c)$和一个参数$\phi$用于表示$Z_{\theta}$的对数<br>$$P_\theta(w|c)&#x3D;P_{\theta^0}(w|c)\exp(\phi)$$<br>那么 参数 $\theta &#x3D; {\theta^0,\phi}$<br>每对一个上下文 $c$ 都需要学习一个对应的 $\phi$，这使得难以扩展到具有大规模上下文的情况。</p>
<h4 id="1-3-1-negative-sampling-负采样"><a href="#1-3-1-negative-sampling-负采样" class="headerlink" title="1.3.1. negative sampling 负采样"></a>1.3.1. negative sampling 负采样</h4><p>论文发现将$Z_{\theta}$固定为1效果也很好，使用 $Z_{\theta}&#x3D;1$ 时的$J^c(\theta)$ 作为目标函数的方法为称为 负采样。</p>
<p>比如用负采样改进的了word2vec<br>$$\begin{aligned}P(D&#x3D;0\mid w,c)&amp;&#x3D;\frac{1}{u_\theta(w,c)+1}\P(D&#x3D;1\mid w,c)&amp;&#x3D;\frac{u_\theta(w,c)}{u_\theta(w,c)+1}.\end{aligned}$$<br>$u_\theta(w,c) &#x3D; \exp{s_\theta(w,c)}$</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1410.8251">Notes on Noise Contrastive Estimation and Negative Sampling</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/MarisaMagic/p/17949927">[NLP复习笔记] Word2Vec: 基于负采样的 Skip-gram 及其 SGD 训练 - 博客园</a></p>
<h3 id="1-4-复杂度"><a href="#1-4-复杂度" class="headerlink" title="1.4. 复杂度"></a>1.4. 复杂度</h3><p>假设$c$是上下文大小，$d$是单词特征向量维度，$V$是模型的词汇量大小。</p>
<p>利用公式计算预测表示，NCE和ML学习都需要进行$cd^2$操作。<br>对于ML，从预测的表示中计算下一个单词的分布大约需要$Vd$个操作。<br>对于NCE，在k个噪声样本下的分类为正样本的概率大约需要$kd$次操作<br>由于 k&lt;&lt;|V|，所以NCE大大提升了计算速度</p>
<h3 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5. 总结"></a>1.5. 总结</h3><p>总结一下，NCE做了两件事</p>
<ul>
<li>更改了目标函数，任务从多分类问题到二分类问题</li>
<li>验证了 $Z_{\theta}$ 在基于NCE的训练中可以直接设为1</li>
</ul>
<h2 id="2-扩展阅读，另一个博主的推导"><a href="#2-扩展阅读，另一个博主的推导" class="headerlink" title="2. 扩展阅读，另一个博主的推导"></a>2. 扩展阅读，另一个博主的推导</h2><p>感觉还是原论文里写的更精炼</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1yqcEeWEyc?spm_id_from=333.788.videopod.sections&vd_source=5b329c82286a01997454e14991ec6231">NCE噪声对比估计_哔哩哔哩_bilibili</a>中对NCE的推导：<br>在给定 $c$ 的情况下，正负样本的概率分别为<br>$$\begin{aligned}P(d&#x3D;1,w|c):&#x3D;P(w|d&#x3D;1,c)P(d&#x3D;1|c)&amp;&#x3D;P(w|d&#x3D;1,c)P(d&#x3D;1)\&amp;&#x3D;P(w|d&#x3D;1,c)\frac{1}{1+k}\end{aligned}$$<br>$$\begin{aligned}P(d&#x3D;0,w|c):&#x3D;P(w|d&#x3D;0,c)P(d&#x3D;0|c)&amp;&#x3D;P(w|d&#x3D;0,c)P(d&#x3D;0)\&amp;&#x3D;P(w|d&#x3D;0,c)\frac{k}{1+k}\end{aligned}$$<br>通过对 $d$ 求和，可以得到概率 $P(w|c)$<br>$$\begin{aligned}P(w|c)&#x3D;\sum_dP(d,w|c)&amp;&#x3D;P(d&#x3D;1,w|c)+P(d&#x3D;0,w|c)\&amp;&#x3D;P(w|d&#x3D;1,c)\frac{1}{1+k}+P(w|d&#x3D;0,c)\frac{k}{1+k}\end{aligned}$$</p>
<p>噪声对比估计的目标函数不再是最大化对数似然，而是<br>$$\max\left{\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log P</em>\theta(d&#x3D;1|w,c)\right]+k\mathbb{E}<em>{w\sim P(d&#x3D;0|w,c)}\left[\log P</em>\theta(d&#x3D;0|w,c)\right]\right}$$<br>P(w|d&#x3D;1,c)}其实就是 正样本的分布P_d  P(w|d&#x3D;0,c)}噪声分布 P(w|d&#x3D;0,c)} P_n</p>
<p>展开：<br>$$\begin{aligned}&amp;\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log P</em>\theta(d&#x3D;1|w,c)\right]+k\mathbb{E}<em>{w\sim P(w|d&#x3D;0,c)}\left[\log P</em>\theta(d&#x3D;0|w,c)\right]\<br>&amp;&#x3D;\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log\frac{P</em>\theta(w|d&#x3D;1,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}\right]+k\mathbb{E}<em>{w\sim P(w|d&#x3D;0,c)}\left[\log\frac{kP(w|d&#x3D;0,c)}{P</em>\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}\right]\<br>&amp;&#x3D; \sum_wP(w|d&#x3D;1,c)\frac{kP(w|d&#x3D;0,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)} \frac{\partial}{\partial\theta}\log P_\theta(w|d&#x3D;1,c)-\sum_wP(w|d&#x3D;0,c)\frac{kP_\theta(w|d&#x3D;1,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}\frac{\partial}{\partial\theta}\log P_\theta(w|d&#x3D;1,c)<br>\end{aligned}$$<br>可以证明：<br>当 $k\rightarrow \infty$ 时，并把 $logZ_\theta(c)$ 当做常数 ，有<br>$$\begin{aligned}&amp;\frac{\partial}{\partial\theta}\left[\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log P</em>\theta(d&#x3D;1|w,c)\right]+k\mathbb{E}<em>{w\sim P(w|d&#x3D;0,c)}\left[\log P</em>\theta(d&#x3D;0|w,c)\right]\right]\&amp;&#x3D;\sum_w\left[P(w|d&#x3D;1,c)-P_\theta(w|d&#x3D;1,c)\right]\frac{\partial}{\partial\theta}s_\theta(w,c)\end{aligned}$$<br>可以发现：<br>在这个情况下，最大化噪声对比估计 等价与最大化似然</p>
<p>我们可以用蒙特卡洛采样法去近似期望，即从数据分布中采样m个点，然后从噪声分布中采样n个点<br>$$&#x3D;\frac{1}{m}\sum_{w}\log\frac{P_{\theta}(w|d&#x3D;1,c)}{P_{\theta}(w,|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}+\frac{k}{n}\sum_{w^{-}}\log\frac{kP(w^-|d&#x3D;0,c)}{P_{\theta}(w^-|d&#x3D;1,c)+kP(w^-|d&#x3D;0,c)}$$</p>
<p>当m&#x3D;1,n&#x3D;k。那么为<br>$$\log\frac{P_\theta(w|d&#x3D;1,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}+\sum_{w_-}\log\frac{kP(w|d&#x3D;0,c)}{P_\theta(w|d&#x3D;1,c)+kP(w|d&#x3D;0,c)}$$</p>
<p>负采样是NCE的一种特殊情况，即让归一化项$Z_{\theta}$固定为常数1且令$kP(w|d&#x3D;0,c)&#x3D;1\to P(w|d&#x3D;0,c)&#x3D;\frac1k$,</p>
<p>那么<br>$$\begin{aligned}&amp;\mathbb{E}<em>{w\sim P(w|d&#x3D;1,c)}\left[\log P</em>\theta(d&#x3D;1|w,c)\right]+k\mathbb{E}<em>{w\sim P(w|d&#x3D;0,c)}\left[\log P</em>\theta(d&#x3D;0|w,c)\right]\<br>&amp;&#x3D;\frac{1}{m}\sum_{w}\log\frac{\exp{s_\theta(w,c)}}{\exp{s_\theta(w,c)}+1}+\frac{k}{n}\sum_{w_-}\log\frac{1}{\exp{s_\theta(w,c)}+1}\<br>&amp;&#x3D;\frac{1}{m}\sum_{w}\log\frac{\exp{s_{\theta}(w,c)}&#x2F;\exp{s_{\theta}(w,c)}}{(\exp{s_{\theta}(w,c)}+1)&#x2F;\exp{s_{\theta}(w,c)}}+\frac{k}{n}\sum_{w}\log\frac{1}{\exp{s_{\theta}(w,c)}+1}\<br>&amp;&#x3D;\frac{1}{m}\sum_{w}\log\frac{1}{1+\exp{-s_{\theta}(w,c)}}+\frac{k}{n}\sum_{w}\log\frac{1}{\exp{s_{\theta}(w,c)}+1}\<br>&amp;&#x3D;\frac{1}{m}\sum_{w}\log\sigma(s_{\theta}(w,c))+\frac{k}{n}\sum_{w}\log\sigma(-s_{\theta}(w,c))<br>\end{aligned}$$</p>
<p>当m&#x3D;1,n&#x3D;k,则<br>$$&#x3D;\log\sigma(s_\theta(w,c))+\sum\log\sigma(-s_\theta(w,c))$$</p>
<p>loss<br>$$<br>-\log\sigma(s_\theta(w,c))-\sum\log\sigma(-s_\theta(w,c))<br>$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E5%99%AA%E5%A3%B0%E5%AF%B9%E6%AF%94%E4%BC%B0%E8%AE%A1-NCE/" data-id="cmeqis6j70000ucv1aeif4q86" data-title="噪声对比估计-NCE" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-web/framework/category test" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/08/11/web/framework/category%20test/" class="article-date">
  <time class="dt-published" datetime="2025-08-11T02:09:10.000Z" itemprop="datePublished">2025-08-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/web/">web</a>►<a class="article-category-link" href="/categories/web/framework/">framework</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/08/11/web/framework/category%20test/">category test</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><img src="/images/category%20test/image-20250811102632139.png"></p>
<p>本文件在source&#x2F;_posts&#x2F;web&#x2F;framework目录下</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/08/11/web/framework/category%20test/" data-id="cme1g4kc200031gv197y08rn4" data-title="category test" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-web/framework/husky test" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/08/11/web/framework/husky%20test/" class="article-date">
  <time class="dt-published" datetime="2025-08-11T01:08:37.000Z" itemprop="datePublished">2025-08-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/web/">web</a>►<a class="article-category-link" href="/categories/web/framework/">framework</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/08/11/web/framework/husky%20test/">husky test</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><img src="/images/husky%20test/image-20250811100826480.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/08/11/web/framework/husky%20test/" data-id="cme1g4kc300041gv1dva8c083" data-title="husky test" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-front matter test" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/08/07/front%20matter%20test/" class="article-date">
  <time class="dt-published" datetime="2025-08-07T13:09:03.000Z" itemprop="datePublished">2025-08-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/08/07/front%20matter%20test/">front matter test</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/08/07/front%20matter%20test/" data-id="cme1g4kbz00011gv15zup0u74" data-title="front matter test" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/08/07/hello-world/" class="article-date">
  <time class="dt-published" datetime="2025-08-07T13:00:43.028Z" itemprop="datePublished">2025-08-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/08/07/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/08/07/hello-world/" data-id="cme1g4kbu00001gv12fkyh03y" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-我的第一篇hexo博客" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/08/01/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87hexo%E5%8D%9A%E5%AE%A2/" class="article-date">
  <time class="dt-published" datetime="2025-08-01T08:53:05.000Z" itemprop="datePublished">2025-08-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/08/01/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87hexo%E5%8D%9A%E5%AE%A2/">我的第一篇hexo博客</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="这是标题"><a href="#这是标题" class="headerlink" title="这是标题"></a>这是标题</h3><p>下面是一个简单的python程序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello, welcome to Rich&#x27;s website&quot;</span>)</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/08/01/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87hexo%E5%8D%9A%E5%AE%A2/" data-id="cme1g4kc000021gv1hj440oko" data-title="我的第一篇hexo博客" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/web/">web</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/web/framework/">framework</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">3.对比学习</a></li></ul></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/08/">August 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-InfoNCE/">损失函数-InfoNCE</a>
          </li>
        
          <li>
            <a href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/%E5%99%AA%E5%A3%B0%E5%AF%B9%E6%AF%94%E4%BC%B0%E8%AE%A1-NCE/">噪声对比估计-NCE</a>
          </li>
        
          <li>
            <a href="/2025/08/11/web/framework/category%20test/">category test</a>
          </li>
        
          <li>
            <a href="/2025/08/11/web/framework/husky%20test/">husky test</a>
          </li>
        
          <li>
            <a href="/2025/08/07/front%20matter%20test/">front matter test</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>